{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwWPSYKD0GoQ",
        "outputId": "4ef9ec5f-95d4-447d-fcbc-c82d3bd73167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cs484'...\n",
            "remote: Enumerating objects: 301, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 301 (delta 8), reused 16 (delta 5), pack-reused 282\u001b[K\n",
            "Receiving objects: 100% (301/301), 83.51 MiB | 17.68 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf cs484 && git clone https://github.com/berkan-sahin/cs484.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nvh0OLr9c2j",
        "outputId": "ded90473-c418-493d-9970-2acfc9e680e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
        "import time\n",
        "import copy\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmY-amFpSQm8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HockeyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transformer) -> None:\n",
        "        self.data_dir = data_dir\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return 192\n",
        "\n",
        "    def __getitem__(self, index) -> Tuple[any, any]:\n",
        "        class_idx = index / 64\n",
        "        if (class_idx < 1):\n",
        "            class_name = 'freehit'\n",
        "        elif (class_idx < 2):\n",
        "            class_name = 'goal'\n",
        "        elif (class_idx < 3):\n",
        "            class_name = 'penaltycorner'\n",
        "        else:\n",
        "            class_name = 'penaltyshot'  # should never happen\n",
        "\n",
        "        img_name = os.path.join(self.data_dir, class_name,\n",
        "                                f'{(index % 64) + 1}.jpg')\n",
        "        image = read_image(img_name).to(torch.float32)\n",
        "        image = self.transformer(image)\n",
        "        return image, class_idx\n",
        "\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.Resize(256, antialias=True),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    # transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "rmyKvWKX7ive"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- modified the learning rate (from 0.0001 to 0.001)\n",
        "- using pytorch's pretrained weights\n",
        "- freezing the feature detection layers\n",
        "- bugfixes\n",
        "- batch size incremented to 32 from 4"
      ],
      "metadata": {
        "id": "LJ1LgTGXRGmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:    \n",
        "    fold = 6\n",
        "    epochs = 50\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device: \", device)\n",
        "    #vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "    vgg16 = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
        "    # vgg16.load_state_dict(torch.load('vgg16.pth'))\n",
        "    # reset the last layer\n",
        "    # Freeze training for all layers\n",
        "    for param in vgg16.features.parameters():\n",
        "      param.require_grad = False\n",
        "\n",
        "    in_features = vgg16.classifier[-1].in_features\n",
        "    classifier = list(vgg16.classifier.children())[:-1]\n",
        "    classifier.extend([nn.Linear(in_features, 3)])\n",
        "    vgg16.classifier = nn.Sequential(*classifier)\n",
        "    print(vgg16.classifier)\n",
        "    vgg16 = vgg16.to(device)\n",
        "    if torch.cuda.is_available():\n",
        "        vgg16.cuda()\n",
        "    dataset = HockeyDataset(\n",
        "        'cs484/dataset', models.VGG16_Weights.IMAGENET1K_V1.transforms(antialias=True))\n",
        "    # kfold = KFold(n_splits=fold, shuffle=True)\n",
        "    kfold = StratifiedKFold(n_splits=fold, shuffle=True, random_state=3)\n",
        "\n",
        " #   train_size = int(0.8 * len(dataset))\n",
        " #   test_size = len(dataset) - train_size\n",
        " #   train, test = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    begin = time.time()\n",
        "    initial_weight = copy.deepcopy(vgg16.state_dict())\n",
        "    accuracy = []\n",
        "    best_epochs = []\n",
        "    for (fold, (train, test)) in enumerate(kfold.split(dataset, np.full(64, 0).tolist() + np.full(64, 1).tolist() + np.full(64, 2).tolist())):\n",
        "        print(\"Train: \", train, \"Validation: \", test)\n",
        "        trainloader = DataLoader(dataset, batch_size=40, sampler=train, num_workers=2)\n",
        "        valloader = DataLoader(dataset, batch_size=40, sampler=test, num_workers=2)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(vgg16.parameters(), lr=0.002)\n",
        "        #optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "        scheduler = optim.lr_scheduler.StepLR(\n",
        "               optimizer, step_size=25, gamma=0.85)\n",
        "\n",
        "        vgg16.load_state_dict(initial_weight)\n",
        "        best_weight = copy.deepcopy(initial_weight)\n",
        "        best_acc = 0.0\n",
        "        best_epoch = 0\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            vgg16.train()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            for inputs, labels in trainloader:\n",
        "                labels = labels.type(torch.LongTensor)\n",
        "                #inputs = inputs.to(device)\n",
        "                #labels = labels.to(device)\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "                else:\n",
        "                    inputs, labels = Variable(inputs), Variable(labels)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = vgg16(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                #print(type(labels))\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                scheduler.step()\n",
        "\n",
        "                del inputs, outputs, labels, preds\n",
        "\n",
        "            epoch_loss = running_loss / len(train)\n",
        "            epoch_acc = running_corrects.double() / len(train)\n",
        "            print('Epoch: {} train Loss: {:.4f} Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            vgg16.eval()\n",
        "            with torch.no_grad():\n",
        "                for images, labels in valloader:\n",
        "                    labels = labels.type(torch.LongTensor)\n",
        "                    optimizer.zero_grad()\n",
        "                    #images = images.to(device)\n",
        "                    #labels = labels.to(device)\n",
        "\n",
        "                    if torch.cuda.is_available():\n",
        "                      images, labels = Variable(images.cuda()), Variable(labels.cuda())\n",
        "                    else:\n",
        "                      images, labels = Variable(images), Variable(labels)\n",
        "                    outputs = vgg16(images)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    running_loss += loss.item() * images.size(0)\n",
        "                    running_corrects += torch.sum(predicted == labels.data)\n",
        "            \n",
        "            epoch_loss = running_loss / len(test)\n",
        "            epoch_acc = running_corrects.double() / len(test)\n",
        "            print('Epoch: {} eval Loss: {:.4f} Acc: {:.4f}'.format(epoch, epoch_loss, epoch_acc))\n",
        "\n",
        "            if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_weight = copy.deepcopy(vgg16.state_dict())\n",
        "                best_epoch = epoch\n",
        "        \n",
        "        accuracy.append(best_acc)\n",
        "        best_epochs.append(best_epoch)\n",
        "        print(f\"Fold {fold} accuracy: {best_acc} achieved in epoch {best_epoch}\")\n",
        "        #torch.save(best_weight, f\"vgg16_fold{fold}.pth\")\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - begin\n",
        "    print(f\"Finished Training, took {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Average accuracy: {sum(accuracy) / len(accuracy)}\")\n",
        "    print(f\"Average epoch: {sum(best_epochs) / len(best_epochs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQz9chVi7p6O",
        "outputId": "148bf117-1a46-482e-acb9-6e1c4349e246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda:0\n",
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=3, bias=True)\n",
            ")\n",
            "Train:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  35  37  39  40  41  42\n",
            "  43  44  45  46  47  48  49  50  51  52  53  55  56  59  61  62  63  64\n",
            "  65  67  69  71  72  74  75  76  77  78  79  80  81  84  86  87  88  89\n",
            "  90  92  93  94  95  96  97  98  99 101 102 103 104 105 107 108 109 110\n",
            " 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 131\n",
            " 132 133 135 136 137 138 139 140 141 143 144 145 146 147 148 149 150 152\n",
            " 153 154 155 156 157 159 160 161 162 163 164 165 166 167 169 170 171 172\n",
            " 173 174 175 176 178 180 181 182 183 184 185 186 188 189 190 191] Validation:  [  0  14  16  28  34  36  38  54  57  58  60  66  68  70  73  82  83  85\n",
            "  91 100 106 111 129 130 134 142 151 158 168 177 179 187]\n",
            "Epoch: 0 train Loss: 44.9665 Acc: 0.3500\n",
            "Epoch: 0 eval Loss: 1.1268 Acc: 0.3438\n",
            "Epoch: 1 train Loss: 1.3286 Acc: 0.2250\n",
            "Epoch: 1 eval Loss: 1.1802 Acc: 0.3125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "avg. accuracies (step size 7, gamma 0.1)\n",
        "- 50 epochs, batch size 32, lr 0.001 : 0.4\n",
        "- 50 epochs, batch size 32, lr 0.0005: 0.4\n",
        "- 50 epochs, batch size 16, lr 0.001: 0.38\n",
        "- VGG16_bn, 50 epochs, batch size 32, lr: 0.001: 0.43\n",
        "- VGG16_bn, 50 epochs, batch size 32, lr: 0.004: 0.39\n",
        "\n",
        "avg. accuracies (step size 10, gamma 0.05)\n",
        "- VGG16_bn, 50 epochs, batch size 32, lr: 0.001: 0.56\n",
        "- VGG16_bn, 100 epochs, batch size 32, lr: 0.001, step size: 15, gamma: 0.05: 0.49\n",
        "- VGG16_bn, 50 epochs, batch size 32, lr: 0.001, no LR scheduler: 0.78\n",
        "- VGG16_bn, 50 epochs, batch size 40, lr: 0.001, no LR scheduler [note: overfitting after epoch 26]: 0.807\n",
        "- VGG16_bn, 50 epochs, batch size 40, lr: 0.001, step size: 25, gamma: 0.05: 0.67\n",
        "- VGG16_bn, 50 epochs, batch size 40, lr: 0.001, step size: 25, gamma: 0.90: 0.807\n",
        "- VGG16_bn, 50 epochs, batch size 40, lr: 0.001, step size: 25, gamma: 0.95: 0.807\n",
        "- VGG16_bn, 100 epochs, batch size 40: lr: 0.002, step size: 25, gamma: 0.95: 0.791\n",
        "- VGG16_bn, 50 epochs, batch size 40: lr: 0.002, step size: 25, gamma: 0.85: ??\n",
        "- **VGG16_bn, 100 epochs, batch size 40, lr: 0.001, step size: 25, gamma: 0.95: 0.848**\n",
        "- VGG16_bn, 50 epochs, batch size 40, lr: 0.001, step size: 25, gamma: 0.85: 0.76\n"
      ],
      "metadata": {
        "id": "p5GYTWjSD_6y"
      }
    }
  ]
}